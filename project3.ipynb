{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Math 5750/6880: Mathematics of Data Science \\\n",
        "Project 3"
      ],
      "metadata": {
        "id": "0gdC70xxFyc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.datasets import load_iris, make_moons, load_breast_cancer, fetch_california_housing\n",
        "from sklearn.datasets import get_data_home\n",
        "# from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy import io #will use this to load matlab data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ],
      "metadata": {
        "id": "-VWWBb5BFfkR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Fashion-MNIST image classification using sklearn"
      ],
      "metadata": {
        "id": "i9_7SnpMGKDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train1, y_train1), (X_test1, y_test1) = fashion_mnist.load_data()\n",
        "X_train1 = X_train1.reshape(len(X_train1), -1)\n",
        "X_test1  = X_test1.reshape(len(X_test1), -1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train1 = scaler.fit_transform(X_train1)\n",
        "X_test1 = scaler.transform(X_test1)"
      ],
      "metadata": {
        "id": "AB136H0PGKq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3a75389-8966-4873-e5fb-6defacd285dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# your code here"
      ],
      "metadata": {
        "id": "5GAsN-dmHjRM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html"
      ],
      "metadata": {
        "id": "gaJ2bg-nPAlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Fashion-MNIST image classification  using pytorch"
      ],
      "metadata": {
        "id": "a2qcKggmIH8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Load Fashion-MNIST\n",
        "# Classes (0-9): T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# scale to [0,1], add channel dimension -> (N, 1, 28, 28)# Adding a channel dimension changes image tensors from (N, 28, 28) to (N, 1, 28, 28), matching CNN input shape requirements.\n",
        "#1 表示通道数（channel），即每张图像的特征图数量。灰度图像只有 1 个通道，RGB 彩色图像有 3 个通道。类似线性代数中的基向量数量：每个通道是一维颜色基，RGB 用三个基线性组合表示颜色。\n",
        "X_train = (X_train.astype(\"float32\") / 255.0)[:, None, :, :]\n",
        "X_test  = (X_test.astype(\"float32\")  / 255.0)[:,  None, :, :]\n",
        "\n",
        "y_train = y_train.astype(np.int64)\n",
        "y_test  = y_test.astype(np.int64)\n",
        "\n",
        "# train/val split: last 10k of train as validation\n",
        "X_tr, X_val = X_train[:50000], X_train[50000:]\n",
        "y_tr, y_val = y_train[:50000], y_train[50000:]\n",
        "\n",
        "# wrap in PyTorch TensorDatasets and DataLoaders\n",
        "train_ds = TensorDataset(torch.from_numpy(X_tr),  torch.from_numpy(y_tr)) # Tensors can be created from NumPy arrays: torch.from_numpy(np_array)\n",
        "val_ds   = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val)) # DataLoader is an iterable that abstracts this complexity for us in an easy API.\n",
        "test_ds  = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))# DataLoader 是一个可迭代对象，通过简单的 API 帮助我们自动处理数据加载的复杂过程，方便高效地读取批量数据\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)# A batch is like a small “sub-dataset” sampled from the full dataset that the model processes together in one training step.\n",
        "# shuffle=True randomizes data order each epoch, preventing the model from learning sequence patterns and improving generalization during training.\n",
        "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=256, shuffle=False)"
      ],
      "metadata": {
        "id": "B9IQwhgcIVOl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# In colab, you should ``change runtime type'' to GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# your code here"
      ],
      "metadata": {
        "id": "0REsDBunNmEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74ffd6c-7b4e-450e-e82e-c5377100c18f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "uV05LjspUHzS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aU0jG1xUNzwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"MLP\" for adm and...."
      ],
      "metadata": {
        "id": "285XH4FHN0Ka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://docs.pytorch.org/docs/stable/nn.html"
      ],
      "metadata": {
        "id": "p8zzIe07SHO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "zVfFr1EyOEQJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "define models"
      ],
      "metadata": {
        "id": "XIBTxJxwqBb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module): # inherent from nn.Modle\n",
        "  def __init__(self):\n",
        "    super().__init__() #  calls nn.Module constructor\n",
        "    self.fc1= nn.Linear(28*28, 256) #same indent as calling the parent constructor, 1st fully connected linear layer\n",
        "    #  also implicitly defines the input dimension\n",
        "    self.fc2= nn.Linear(256,128) # By default, nn.Linear uses bias=True\n",
        "    self.fc3=nn.Linear(128,60)\n",
        "    self.fc4 = nn.Linear(60, 10) # using simialr structure as before\n",
        "  def forward(self, x):\n",
        "      x= x.view(-1, 28*28)  # flatten\n",
        "      x= torch.relu(self.fc1(x))\n",
        "      x= torch.tanh(self.fc2(x))\n",
        "      x= torch.relu(self.fc3(x))\n",
        "      return torch.softmax(self.fc4(x), dim=1) # foward process finally give predictions\n"
      ],
      "metadata": {
        "id": "knYKC5e4OIIz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_MLP1 = MLPModel()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_MLP1.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PztZbDXRqKXx",
        "outputId": "9b8e1127-3d87-40d0-dd68-9c9a8b011b62"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPModel(\n",
              "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=60, bias=True)\n",
              "  (fc4): Linear(in_features=60, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define optimizater and loss"
      ],
      "metadata": {
        "id": "eVCR7ZD7qhzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion1 = nn.CrossEntropyLoss()\n",
        "#optimizer1 = torch.optim.LBFGS(\n",
        "    #model_MLP1.parameters(),\n",
        "    #lr=0.01,          # moderate learning rate\n",
        "    #max_iter=160,    # enough inner steps to converge per batch\n",
        "    #history_size=20, # keeps past updates for curvature estimation\n",
        "    #line_search_fn='strong_wolfe'  # improves stability\n",
        "#)\n"
      ],
      "metadata": {
        "id": "lfcXcZWpqoN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_1 = torch.optim.Adam(\n",
        "    model_MLP1.parameters(),\n",
        "    lr=0.0005,          # standard learning rate\n",
        "    betas=(0.9, 0.999), # momentum terms for first and second moments\n",
        "    eps=1e-7,          # small constant for numerical stability\n",
        "    weight_decay=0.0006     # optional L2 regularization\n",
        ")\n"
      ],
      "metadata": {
        "id": "qultXZMq-Xs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "XIyaNCqOt0gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "qMAcEscg5Kwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, loader, optimizer, criterion, epoch):\n",
        "    start = time.time()\n",
        "    model.train() # set in train mode\n",
        "    for batch_idx, (data, target) in enumerate(loader): # data like input, target like true output labels\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Closure required for L-BFGS\n",
        "        def closure():\n",
        "            optimizer.zero_grad() #梯度清零\n",
        "            output = model(data) # forward pass\n",
        "            loss = criterion(output, target) # sum loss\n",
        "            loss.backward()   # backward pass\n",
        "            return loss\n",
        "\n",
        "        loss = optimizer.step(closure) # update parameters by optimize\n",
        "\n",
        "        if batch_idx % 50 == 0: # report some eporch process and loss per 50 batch\n",
        "            print(f\"Epoch {epoch} [{batch_idx * len(data)}/{len(loader.dataset)}]\"\n",
        "                  f\" Loss: {loss.item():.6f}\")\n",
        "    end = time.time()\n",
        "    print(f\"Epoch {epoch} Time: {end-start:.2f}s\")"
      ],
      "metadata": {
        "id": "kdkxGcGAt2FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate and test"
      ],
      "metadata": {
        "id": "chbyHTitxnWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, loader, criterion):\n",
        "    model.eval() # set in evaluate mode\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad(): # prohibit gradient calculation\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data) # forward pass\n",
        "            test_loss += criterion(output, target).item() # sum loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get predictions\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item() # # find the numeber of correct prediction \\\n",
        "    acc = 100. * correct / len(loader.dataset)\n",
        "    print(f\"Average loss: {test_loss/len(loader.dataset):.6f}, validate accuracy: {acc:.2f}%\") # (e.g., 0.87) to a percentage (87%)\n",
        "    return acc"
      ],
      "metadata": {
        "id": "Gj1zKrxixm8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run train and test"
      ],
      "metadata": {
        "id": "G-px-fDRziln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader2 = DataLoader(train_ds, batch_size=len(train_ds), shuffle=True)\n"
      ],
      "metadata": {
        "id": "S3luDMJR6B8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10  # L-BFGS is slower per step; fewer epochs\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model_MLP1, device, train_loader, optimizer_1, criterion1, epoch)\n",
        "    test(model_MLP1, device, val_loader, criterion1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wW2sHDJKzcJO",
        "outputId": "9f6140d7-c6cc-4dd9-d4ae-4528e1afb4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 [0/50000] Loss: 2.302278\n",
            "Epoch 1 [6400/50000] Loss: 1.969210\n",
            "Epoch 1 [12800/50000] Loss: 1.842707\n",
            "Epoch 1 [19200/50000] Loss: 1.770039\n",
            "Epoch 1 [25600/50000] Loss: 1.708390\n",
            "Epoch 1 [32000/50000] Loss: 1.646204\n",
            "Epoch 1 [38400/50000] Loss: 1.635658\n",
            "Epoch 1 [44800/50000] Loss: 1.712749\n",
            "Epoch 1 Time: 1.04s\n",
            "Average loss: 0.006679, validate accuracy: 79.70%\n",
            "Epoch 2 [0/50000] Loss: 1.647282\n",
            "Epoch 2 [6400/50000] Loss: 1.706715\n",
            "Epoch 2 [12800/50000] Loss: 1.670159\n",
            "Epoch 2 [19200/50000] Loss: 1.707644\n",
            "Epoch 2 [25600/50000] Loss: 1.638750\n",
            "Epoch 2 [32000/50000] Loss: 1.617531\n",
            "Epoch 2 [38400/50000] Loss: 1.693098\n",
            "Epoch 2 [44800/50000] Loss: 1.655381\n",
            "Epoch 2 Time: 1.05s\n",
            "Average loss: 0.006669, validate accuracy: 79.92%\n",
            "Epoch 3 [0/50000] Loss: 1.657424\n",
            "Epoch 3 [6400/50000] Loss: 1.672896\n",
            "Epoch 3 [12800/50000] Loss: 1.643240\n",
            "Epoch 3 [19200/50000] Loss: 1.705787\n",
            "Epoch 3 [25600/50000] Loss: 1.628467\n",
            "Epoch 3 [32000/50000] Loss: 1.695850\n",
            "Epoch 3 [38400/50000] Loss: 1.628603\n",
            "Epoch 3 [44800/50000] Loss: 1.670509\n",
            "Epoch 3 Time: 1.05s\n",
            "Average loss: 0.006636, validate accuracy: 80.64%\n",
            "Epoch 4 [0/50000] Loss: 1.641560\n",
            "Epoch 4 [6400/50000] Loss: 1.661171\n",
            "Epoch 4 [12800/50000] Loss: 1.648295\n",
            "Epoch 4 [19200/50000] Loss: 1.610192\n",
            "Epoch 4 [25600/50000] Loss: 1.636543\n",
            "Epoch 4 [32000/50000] Loss: 1.662559\n",
            "Epoch 4 [38400/50000] Loss: 1.651131\n",
            "Epoch 4 [44800/50000] Loss: 1.631290\n",
            "Epoch 4 Time: 1.05s\n",
            "Average loss: 0.006523, validate accuracy: 83.59%\n",
            "Epoch 5 [0/50000] Loss: 1.702217\n",
            "Epoch 5 [6400/50000] Loss: 1.621552\n",
            "Epoch 5 [12800/50000] Loss: 1.619928\n",
            "Epoch 5 [19200/50000] Loss: 1.614197\n",
            "Epoch 5 [25600/50000] Loss: 1.645769\n",
            "Epoch 5 [32000/50000] Loss: 1.575027\n",
            "Epoch 5 [38400/50000] Loss: 1.678004\n",
            "Epoch 5 [44800/50000] Loss: 1.624231\n",
            "Epoch 5 Time: 1.05s\n",
            "Average loss: 0.006504, validate accuracy: 83.92%\n",
            "Epoch 6 [0/50000] Loss: 1.630907\n",
            "Epoch 6 [6400/50000] Loss: 1.652670\n",
            "Epoch 6 [12800/50000] Loss: 1.568566\n",
            "Epoch 6 [19200/50000] Loss: 1.644679\n",
            "Epoch 6 [25600/50000] Loss: 1.614993\n",
            "Epoch 6 [32000/50000] Loss: 1.616131\n",
            "Epoch 6 [38400/50000] Loss: 1.605238\n",
            "Epoch 6 [44800/50000] Loss: 1.626057\n",
            "Epoch 6 Time: 1.03s\n",
            "Average loss: 0.006478, validate accuracy: 84.49%\n",
            "Epoch 7 [0/50000] Loss: 1.595191\n",
            "Epoch 7 [6400/50000] Loss: 1.606354\n",
            "Epoch 7 [12800/50000] Loss: 1.614289\n",
            "Epoch 7 [19200/50000] Loss: 1.611924\n",
            "Epoch 7 [25600/50000] Loss: 1.579368\n",
            "Epoch 7 [32000/50000] Loss: 1.609201\n",
            "Epoch 7 [38400/50000] Loss: 1.631177\n",
            "Epoch 7 [44800/50000] Loss: 1.605333\n",
            "Epoch 7 Time: 1.02s\n",
            "Average loss: 0.006472, validate accuracy: 84.84%\n",
            "Epoch 8 [0/50000] Loss: 1.591612\n",
            "Epoch 8 [6400/50000] Loss: 1.653136\n",
            "Epoch 8 [12800/50000] Loss: 1.596130\n",
            "Epoch 8 [19200/50000] Loss: 1.629039\n",
            "Epoch 8 [25600/50000] Loss: 1.608604\n",
            "Epoch 8 [32000/50000] Loss: 1.616835\n",
            "Epoch 8 [38400/50000] Loss: 1.579603\n",
            "Epoch 8 [44800/50000] Loss: 1.584007\n",
            "Epoch 8 Time: 1.02s\n",
            "Average loss: 0.006432, validate accuracy: 85.66%\n",
            "Epoch 9 [0/50000] Loss: 1.637130\n",
            "Epoch 9 [6400/50000] Loss: 1.671536\n",
            "Epoch 9 [12800/50000] Loss: 1.574020\n",
            "Epoch 9 [19200/50000] Loss: 1.665383\n",
            "Epoch 9 [25600/50000] Loss: 1.546061\n",
            "Epoch 9 [32000/50000] Loss: 1.589587\n",
            "Epoch 9 [38400/50000] Loss: 1.575819\n",
            "Epoch 9 [44800/50000] Loss: 1.621785\n",
            "Epoch 9 Time: 1.02s\n",
            "Average loss: 0.006461, validate accuracy: 84.89%\n",
            "Epoch 10 [0/50000] Loss: 1.601583\n",
            "Epoch 10 [6400/50000] Loss: 1.598720\n",
            "Epoch 10 [12800/50000] Loss: 1.602853\n",
            "Epoch 10 [19200/50000] Loss: 1.596961\n",
            "Epoch 10 [25600/50000] Loss: 1.641233\n",
            "Epoch 10 [32000/50000] Loss: 1.616782\n",
            "Epoch 10 [38400/50000] Loss: 1.590551\n",
            "Epoch 10 [44800/50000] Loss: 1.605414\n",
            "Epoch 10 Time: 1.03s\n",
            "Average loss: 0.006472, validate accuracy: 84.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5  # L-BFGS is slower per step; fewer epochs\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model_MLP1, device, train_loader, optimizer_1, criterion1, epoch)\n",
        "    test(model_MLP1, device, val_loader, criterion1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_U3RkAmK8Fod",
        "outputId": "ea31d7fb-bb7f-4f9d-ab45-10293ba0d36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 [0/50000] Loss: 1.541251\n",
            "Epoch 1 [6400/50000] Loss: 1.547117\n",
            "Epoch 1 [12800/50000] Loss: 1.561486\n",
            "Epoch 1 [19200/50000] Loss: 1.580660\n",
            "Epoch 1 [25600/50000] Loss: 1.558943\n",
            "Epoch 1 [32000/50000] Loss: 1.578335\n",
            "Epoch 1 [38400/50000] Loss: 1.575950\n",
            "Epoch 1 [44800/50000] Loss: 1.555690\n",
            "Epoch 1 Time: 1.04s\n",
            "Average loss: 0.006383, validate accuracy: 86.79%\n",
            "Epoch 2 [0/50000] Loss: 1.574288\n",
            "Epoch 2 [6400/50000] Loss: 1.615824\n",
            "Epoch 2 [12800/50000] Loss: 1.627423\n",
            "Epoch 2 [19200/50000] Loss: 1.608472\n",
            "Epoch 2 [25600/50000] Loss: 1.551310\n",
            "Epoch 2 [32000/50000] Loss: 1.547161\n",
            "Epoch 2 [38400/50000] Loss: 1.593661\n",
            "Epoch 2 [44800/50000] Loss: 1.636502\n",
            "Epoch 2 Time: 1.06s\n",
            "Average loss: 0.006376, validate accuracy: 87.03%\n",
            "Epoch 3 [0/50000] Loss: 1.633572\n",
            "Epoch 3 [6400/50000] Loss: 1.577116\n",
            "Epoch 3 [12800/50000] Loss: 1.567292\n",
            "Epoch 3 [19200/50000] Loss: 1.558524\n",
            "Epoch 3 [25600/50000] Loss: 1.536494\n",
            "Epoch 3 [32000/50000] Loss: 1.617350\n",
            "Epoch 3 [38400/50000] Loss: 1.586485\n",
            "Epoch 3 [44800/50000] Loss: 1.549945\n",
            "Epoch 3 Time: 1.06s\n",
            "Average loss: 0.006377, validate accuracy: 87.20%\n",
            "Epoch 4 [0/50000] Loss: 1.548395\n",
            "Epoch 4 [6400/50000] Loss: 1.566879\n",
            "Epoch 4 [12800/50000] Loss: 1.577414\n",
            "Epoch 4 [19200/50000] Loss: 1.622053\n",
            "Epoch 4 [25600/50000] Loss: 1.574447\n",
            "Epoch 4 [32000/50000] Loss: 1.574558\n",
            "Epoch 4 [38400/50000] Loss: 1.555344\n",
            "Epoch 4 [44800/50000] Loss: 1.531844\n",
            "Epoch 4 Time: 1.09s\n",
            "Average loss: 0.006351, validate accuracy: 87.75%\n",
            "Epoch 5 [0/50000] Loss: 1.598504\n",
            "Epoch 5 [6400/50000] Loss: 1.578124\n",
            "Epoch 5 [12800/50000] Loss: 1.570753\n",
            "Epoch 5 [19200/50000] Loss: 1.628598\n",
            "Epoch 5 [25600/50000] Loss: 1.559024\n",
            "Epoch 5 [32000/50000] Loss: 1.574205\n",
            "Epoch 5 [38400/50000] Loss: 1.605821\n",
            "Epoch 5 [44800/50000] Loss: 1.589659\n",
            "Epoch 5 Time: 1.05s\n",
            "Average loss: 0.006355, validate accuracy: 87.56%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model_MLP1, device, test_loader, criterion1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez7a5NOlA1qK",
        "outputId": "ba347090-6675-47e9-b422-6957962bca65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average loss: 0.006384, validate accuracy: 86.86%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "86.86"
            ]
          },
          "metadata": {},
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36Rd3uNx10ef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model_MLP1.state_dict(), \"MLPModel_LBFGS.pth\")\n",
        "#print(\"Model saved to MLPModel_LBFGS.pth\")"
      ],
      "metadata": {
        "id": "b5-3_29B0KU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "j4g6T2M0qDPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define CNN model\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        # Input: (N, 1, 28, 28)\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # -> (N, 32, 28, 28)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # -> (N, 32, 14, 14), redcue dim\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) # -> (N, 64, 14, 14)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)                          # -> (N, 64, 7, 7)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create model instance\n",
        "model_1 = CNNModel()\n",
        "print(model_1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5L5LarSSx7f",
        "outputId": "ec2e04d2-160b-4764-9f27-0649391057a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNModel(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=3136, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "uX1DovTm6nfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()  # same as SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer_1 = optim.Adam(model_1.parameters(), lr=0.002)"
      ],
      "metadata": {
        "id": "ecNoTEn2XSzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_1 = model_1.to(device)  #  move model weights to GPU\n",
        "\n",
        "# Get one batch and move to same device\n",
        "start_t_CNN_total_1 = time.time()\n",
        "for epoch in range(5):\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer_1.zero_grad()\n",
        "        preds = model_1(X_batch)\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer_1.step()\n",
        "    print(f\"Epoch {epoch+1}/5 completed\")\n",
        "end_t_CNN_total_1 = time.time()\n",
        "total_time_trained=end_t_CNN_total_1-start_t_CNN_total_1\n",
        "print('total time trained by Pytorch:',total_time_trained)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bIdtmZhY1ph",
        "outputId": "9f9af861-c904-40f1-ce8a-609bfaa2ecc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 completed\n",
            "Epoch 2/5 completed\n",
            "Epoch 3/5 completed\n",
            "Epoch 4/5 completed\n",
            "Epoch 5/5 completed\n",
            "total time trained by Pytorch: 5.940268039703369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score"
      ],
      "metadata": {
        "id": "Q8bEMdG_aIhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== VALIDATION =====\n",
        "model_1.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in val_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        preds = model_1(X_batch)\n",
        "        predicted = preds.argmax(1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(y_batch.cpu().numpy())\n",
        "\n",
        "# Compute metrics\n",
        "acc = accuracy_score(all_labels, all_preds)\n",
        "f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "print(f\"Validation Accuracy: {acc:.4f}\")\n",
        "print(f\"Validation F1 Score (macro): {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCxx280HZ8_s",
        "outputId": "8fe92525-6095-4925-8376-0452858f8367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9225\n",
            "Validation F1 Score (macro): 0.9225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GAEgejWJFzOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW4QMwg5GVfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above 0.92 is fine-tuning on a pre-trained model with 0.9 accuracy."
      ],
      "metadata": {
        "id": "sfqY3nDgFpzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test2 = torch.from_numpy(X_test).float()\n",
        "outputs2 = model_1(X_test2.to(device))\n",
        "y_pred_CNN1 = outputs2.argmax(dim=1).cpu().numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "uvWNTgAOGWFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm_CNN1 = confusion_matrix(y_test, y_pred_CNN1 )\n",
        "print(cm_CNN1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIT_WcTqFobj",
        "outputId": "1ed50405-f372-4ffa-8842-5be30f8c5a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[893   0  14  16   4   1  66   0   6   0]\n",
            " [  2 985   0   7   0   0   5   0   1   0]\n",
            " [ 26   1 856   8  44   0  64   0   1   0]\n",
            " [ 27   2   3 928  28   0  11   0   1   0]\n",
            " [  1   0  36  23 864   0  76   0   0   0]\n",
            " [  0   0   0   0   0 976   0  17   0   7]\n",
            " [127   2  42  25  49   0 750   0   5   0]\n",
            " [  0   0   0   0   0   2   0 985   0  13]\n",
            " [  4   1   1   4   1   1   1   1 986   0]\n",
            " [  1   0   0   0   0   4   0  38   0 957]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred_CNN1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuIZEPnLG-HD",
        "outputId": "9776eb8d-c67e-4196-b14d-a4af1ab9a6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.89      0.86      1000\n",
            "           1       0.99      0.98      0.99      1000\n",
            "           2       0.90      0.86      0.88      1000\n",
            "           3       0.92      0.93      0.92      1000\n",
            "           4       0.87      0.86      0.87      1000\n",
            "           5       0.99      0.98      0.98      1000\n",
            "           6       0.77      0.75      0.76      1000\n",
            "           7       0.95      0.98      0.97      1000\n",
            "           8       0.99      0.99      0.99      1000\n",
            "           9       0.98      0.96      0.97      1000\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TP_vec_Adam = np.diag(cm_CNN1)\n",
        "print(\"True Positives per class:\", TP_vec_Adam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd9ceRX2O8Ob",
        "outputId": "97601c42-b739-4878-bf41-3a3169aeb0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives per class: [893 985 856 928 864 976 750 985 986 957]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_TP_Adam = np.trace(cm_CNN1)\n",
        "total_TP_Adam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIVi1i04PETY",
        "outputId": "885eb6cf-d07c-4445-9454-2ec17cd32291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(9180)"
            ]
          },
          "metadata": {},
          "execution_count": 397
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JM6jLzwKHO-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transfer learning ()"
      ],
      "metadata": {
        "id": "4LkzRXpaNVRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "baby version"
      ],
      "metadata": {
        "id": "ybTeVHOrNt5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "你说：\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms, datasets\n",
        "import time\n",
        "\n",
        "# =====================================\n",
        "# 1. Load and preprocess Fashion-MNIST\n",
        "# =====================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # ResNet expects 224x224\n",
        "    transforms.Grayscale(3),         # convert single-channel to 3\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_ds = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False)\n",
        "\n",
        "# =====================================\n",
        "# 2. Define transfer-learning model\n",
        "# =====================================\n",
        "class TransferResNet18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransferResNet18, self).__init__()\n",
        "        self.base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        for param in self.base.parameters():\n",
        "            param.requires_grad = False  # freeze all pretrained layers\n",
        "        num_features = self.base.fc.in_features\n",
        "        self.base.fc = nn.Linear(num_features, 10)  # new classifier for Fashion-MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base(x)\n",
        "\n",
        "# Instantiate model, loss, optimizer\n",
        "model_1 = TransferResNet18()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_1 = model_1.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer_1 = optim.Adam(model_1.base.fc.parameters(), lr=0.0005)  # fine-tune only last layer\n",
        "\n",
        "# =====================================\n",
        "# 3. Training Loop\n",
        "# =====================================\n",
        "start_t_CNN_total_1 = time.time()\n",
        "for epoch in range(5):\n",
        "    model_1.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer_1.zero_grad()\n",
        "        preds = model_1(X_batch)\n",
        "        loss = loss_fn(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer_1.step()\n",
        "    print(f\"Epoch {epoch+1}/5 completed, Loss: {loss.item():.6f}\")\n",
        "end_t_CNN_total_1 = time.time()\n",
        "\n",
        "total_time_trained = end_t_CNN_total_1 - start_t_CNN_total_1\n",
        "print('Total time trained by PyTorch:', total_time_trained)\n",
        "\n",
        "# =====================================\n",
        "# 4. Evaluate on test set\n",
        "# =====================================\n",
        "model_1.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model_1(X_batch)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += preds.eq(y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "acc = 100.0 * correct / total\n",
        "print(f\"Final Test Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "id": "Zfm_msUyNZWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Truncation (parameter reduction)\n",
        "\n",
        "GAP (Global Average Pooling (GAP)) + ReLU added in forward\n",
        "\n",
        "Dynamic freezing (Stage 1 → Stage 2)"
      ],
      "metadata": {
        "id": "5dvDwjFPN6YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models, transforms, datasets\n",
        "import time\n",
        "\n",
        "# =====================================\n",
        "# 1. Load and preprocess Fashion-MNIST\n",
        "# =====================================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),     # ↓ smaller input → fewer parameters (truncate-friendly)\n",
        "    transforms.Grayscale(3),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_ds = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_ds  = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_ds, batch_size=128, shuffle=False)\n",
        "\n",
        "# =====================================\n",
        "# 2. Define truncated + GAP + dynamic-freeze ResNet18\n",
        "# =====================================\n",
        "class TruncatedResNet18(nn.Module):\n",
        "    def __init__(self, freeze=True):\n",
        "        super().__init__()\n",
        "        base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "        #  Truncation: keep layers conv1 → layer3 only (drop layer4)\n",
        "        # This reduces total parameters & speeds up training.\n",
        "        self.features = nn.Sequential(\n",
        "            base.conv1,\n",
        "            base.bn1,\n",
        "            base.relu,\n",
        "            base.maxpool,\n",
        "            base.layer1,\n",
        "            base.layer2,\n",
        "            base.layer3# only up to layer3\n",
        "        )\n",
        "\n",
        "        # Optional freeze (for Stage 1)\n",
        "        if freeze:\n",
        "            for param in self.features.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        #  Replace top with new, smaller head:\n",
        "        # Global Average Pooling (1×1), ReLU, and Linear classifier.\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling layer\n",
        "        self.relu = nn.ReLU(inplace=True)        # Activation to avoid flat 0s\n",
        "        self.fc = nn.Linear(256, 10)             # Truncated ResNet18 layer3 → 256 features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)     # truncated backbone\n",
        "        x = self.gap(x)          #  GAP: convert to (N, 256, 1, 1)\n",
        "        x = torch.flatten(x, 1)  # flatten to (N, 256)\n",
        "        x = self.relu(x)         #  ReLU activation after pooling (new addition)\n",
        "        x = self.fc(x)           # linear classifier output\n",
        "        return x\n",
        "\n",
        "\n",
        "# =====================================\n",
        "# 3. Dynamic training stages\n",
        "# =====================================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Stage 1: Frozen truncated backbone (transfer learning)\n",
        "model = TruncatedResNet18(freeze=True).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.0005)\n",
        "print(\"Stage 1: Training truncated backbone (frozen conv layers)\")\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/3 — Loss: {loss.item():.4f}\")\n",
        "print(f\"Stage 1 complete — Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "# --- Stage 2: Unfreeze backbone for fine-tuning\n",
        "#  Dynamic unfreezing (now allows all layers to update)\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = True #freeze all layers\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "print(\"\\nStage 2: Fine-tuning truncated model (all layers trainable)\")\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(2):\n",
        "    model.train()\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_batch)\n",
        "        loss = criterion(preds, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/2 — Loss: {loss.item():.4f}\")\n",
        "print(f\"Stage 2 complete — Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "# =====================================\n",
        "# 4. Evaluation\n",
        "# =====================================\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        outputs = model(X_batch)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += preds.eq(y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "acc = 100.0 * correct / total\n",
        "print(f\"\\nFinal Test Accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFSQLK7POI7y",
        "outputId": "3d036225-3040-4e49-a5b7-fb75a390d0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 13.9MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 203kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.76MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 16.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 210MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage 1: Training truncated backbone (frozen conv layers)\n",
            "Epoch 1/3 — Loss: 1.4843\n",
            "Epoch 2/3 — Loss: 1.2033\n",
            "Epoch 3/3 — Loss: 1.0237\n",
            "Stage 1 complete — Time: 135.98s\n",
            "\n",
            "Stage 2: Fine-tuning truncated model (all layers trainable)\n",
            "Epoch 1/2 — Loss: 0.0862\n",
            "Epoch 2/2 — Loss: 0.1333\n",
            "Stage 2 complete — Time: 98.30s\n",
            "\n",
            "Final Test Accuracy: 93.54%\n"
          ]
        }
      ]
    }
  ]
}